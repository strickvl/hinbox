# `hinbox`

A tool for processing and extracting information from articles related to Guantánamo Bay detention facility.

## Overview

This project processes articles to extract relevant information about Guantánamo Bay, including people, events, locations, and organizations mentioned in the articles. It uses LLMs (Gemini or local Ollama models) to analyze articles and build a knowledge base of entities.

## Quick Start

The project provides a user-friendly CLI interface through `run.py` or using `just`:

### Using run.py
```bash
# Process 5 articles with relevance checking
./run.py process --relevance

# Check article statistics
./run.py check

# Start the web interface to browse entities
./run.py frontend
```

### Using just (recommended)
If you have [just](https://github.com/casey/just) installed:

```bash
# Show available commands
just

# Process articles
just process --relevance

# Check article statistics  
just check

# Start the web interface
just frontend
```

## Installation

1. Clone the repository
2. Install dependencies using uv:
   ```bash
   uv sync
   ```
3. Set up your API keys in environment variables:
   - `GEMINI_API_KEY` for Google Gemini
   - `OLLAMA_API_URL` for local Ollama (optional)

## Commands

### Process Articles

Process articles from the parquet file and extract entities:

```bash
./run.py process [options]
```

Options:
- `-n, --limit N`: Number of articles to process (default: 5)
- `--local`: Use local models (Ollama/spaCy) instead of cloud APIs
- `--relevance`: Perform relevance check before processing
- `--force`: Force reprocessing of already processed articles
- `--articles-path PATH`: Custom path to articles parquet file
- `-v, --verbose`: Enable verbose logging

Examples:
```bash
# Process 10 articles with relevance checking
./run.py process -n 10 --relevance

# Force reprocess using local models
./run.py process --force --local

# Process with verbose logging
./run.py process -v --relevance
```

### Check Article Statistics

Display statistics about articles in the database:

```bash
./run.py check [--sample]
```

Options:
- `--sample`: Display a sample article

### Web Interface

Start the FastHTML web interface to browse extracted entities:

```bash
./run.py frontend
# or
./run.py web
# or
./run.py ui
```

The interface will be available at http://localhost:5001

### Reset Processing Status

Reset the processing status of all articles:

```bash
./run.py reset
```

This will prompt for confirmation before resetting.

### Miami Herald Commands

Fetch and import Miami Herald articles:

```bash
# Fetch new articles
./run.py fetch-miami

# Import articles from JSONL
./run.py import-miami
```

## Project Structure

```
hinbox/
├── run.py                 # Main CLI interface
├── src/
│   ├── process_and_extract.py  # Main processing script
│   ├── merge.py           # Entity merging logic
│   ├── people.py          # Person extraction
│   ├── organizations.py   # Organization extraction
│   ├── locations.py       # Location extraction
│   ├── events.py          # Event extraction
│   ├── relevance.py       # Relevance checking
│   ├── profiles.py        # Entity profile generation
│   ├── embeddings.py      # Text embedding utilities
│   ├── logging_config.py  # Centralized logging
│   └── frontend/          # Web interface
├── data/
│   ├── raw_sources/       # Raw article data
│   │   └── miami_herald_articles.parquet
│   └── entities/          # Extracted entities
│       ├── people.parquet
│       ├── organizations.parquet
│       ├── locations.parquet
│       └── events.parquet
└── scripts/               # Utility scripts
```

## Output Files

The processing creates parquet files for each entity type:

- `data/entities/people.parquet`: People mentioned in articles
- `data/entities/organizations.parquet`: Organizations mentioned
- `data/entities/locations.parquet`: Locations referenced
- `data/entities/events.parquet`: Events described

Each entity includes:
- Profile text generated by LLM
- Source article information
- Processing metadata
- Embeddings for similarity matching

## Features

- **Intelligent Entity Extraction**: Uses LLMs to understand context and extract relevant entities
- **Entity Deduplication**: Uses embeddings to identify and merge duplicate entities
- **Relevance Filtering**: Only processes articles relevant to Guantánamo Bay
- **Profile Generation**: Creates comprehensive profiles for each entity
- **Web Interface**: Browse and search extracted entities
- **Flexible Processing**: Support for both cloud (Gemini) and local (Ollama) models

## Development

### Running Tests

```bash
# Format code
./scripts/format.sh

# Run linting
./scripts/lint.sh
```

### Logging

The project uses a centralized logging system with Rich formatting. Log levels include:
- `info`: General information
- `warning`: Important notices
- `error`: Error messages
- `success`: Successful operations
- `processing`: Processing status updates

Enable verbose logging with the `-v` flag when processing articles.